{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58684051-f433-4c48-ab4a-d51dbf16e694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "from src import preprocess\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4ebc3d2-d9e3-4af8-b4d3-e2ad07e080b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7c08cfc-ad32-4113-941f-81998efcc8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data/\"\n",
    "CLINICAL_NOTES_FILE = DATA_DIR + \"ClinNotes.csv\"\n",
    "MEDICAL_CONCEPTS_FILE = DATA_DIR + \"MedicalConcepts.csv\"\n",
    "\n",
    "PROCESSDED_DATA_DIR = './processed_data/'\n",
    "PROCESSED_CLINICAL_NOTES_FILE = PROCESSDED_DATA_DIR + \"ClinNotes.csv\"\n",
    "NORMALIZED_CLINICAL_NOTES_FILE = PROCESSDED_DATA_DIR + \"ClinNotes_normalized.csv\"\n",
    "BIOWORDVEC_VECTOR_FILE = PROCESSDED_DATA_DIR + 'BioWordVec_vector.npy'\n",
    "\n",
    "WORD_VEC_DIR = './word_vector/'\n",
    "BIOWORDVEC_MODEL_FILE = WORD_VEC_DIR + 'BioWordVec_PubMed_MIMICIII_d200.vec.bin'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da165970-325c-44eb-92b2-d36da1c3b1a3",
   "metadata": {},
   "source": [
    "# Word Vector Aggregation Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ff3510-8f5e-454a-b5c5-e902cac9513c",
   "metadata": {},
   "source": [
    "In this notebook we will use Word Vector Aggregation to vectorize our clinical notes. Word vectors are pre-trained on large corpus to capture the semantic meaning and we can use the word vector pre-trained on medical corpus in this project. The word vector I decide to use is [BioWordVec](https://github.com/ncbi-nlp/BioSentVec).\n",
    "\n",
    "Although the word sequence does not matter in this method, I'm not going to include the related medical terms as TF-IDF since I believe the word vector itself should already encode the semantic correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9ff6b6f-b041-414e-a5f3-9e3f27ed6e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clinical_normalized = pd.read_csv(NORMALIZED_CLINICAL_NOTES_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d44f47-6932-4992-bce6-14cf2085b836",
   "metadata": {},
   "source": [
    "As the word vector file is quite big, I didn't inlcude it on Github. It can be downloaded from [here](https://github.com/ncbi-nlp/BioSentVec#biowordvec-1-biomedical-word-embeddings-with-fasttext)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ac452e1-fb5b-406b-a63e-e3a2c587f8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3.78 s\n",
      "Wall time: 3.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(BIOWORDVEC_MODEL_FILE, binary=True, limit=500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4f8121-b432-43f6-9e6e-dee78a3e4bd6",
   "metadata": {},
   "source": [
    "There are a few aggregation methods, in this project we just go with averaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33dd7255-31c8-461e-9404-17f77e634bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors, num_word, num_oov = preprocess.get_aggregated_doc_vector(model, df_clinical_normalized['notes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dd60622-c464-4d65-ad08-3a74b115ee22",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(PROCESSDED_DATA_DIR).mkdir(parents=True, exist_ok=True)\n",
    "np.save(BIOWORDVEC_VECTOR_FILE, vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa40e3ec-c523-4a12-81e4-706bea0f74bf",
   "metadata": {},
   "source": [
    "Below we get a sense of how many tokens are OOV in the word vector and it is neglectable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0be0297b-3457-4e6e-9cea-e6a0bf8ced4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOV words has a percentage of 0.98%\n"
     ]
    }
   ],
   "source": [
    "print('OOV words has a percentage of {:.2f}%'.format((num_oov / num_word) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9e60f6-3c91-4e1c-a8bf-626a1cec514a",
   "metadata": {},
   "source": [
    "I would like to mention that I'm aware of the word vector aggregation is a bit crude as it does not care about the word sequence at all. We can overcome this shortcoming by using some unsupervised sentence vectorization method. Actually BioWordVec here provides a BioSentVec method which is built upon the Sent2Vec technique. But my local machine has an C++ error when installing Sent2vec package, due to the time constraints I didn't dig in too much and move on with the word vector aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7034eb2-ce6f-4ef2-bbe7-c583df683458",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs5246",
   "language": "python",
   "name": "cs5246"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
